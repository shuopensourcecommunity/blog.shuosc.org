---
title: 深度强化学习
author: shuosc
tags: 活动记录
categories: 社区活动
abbrlink: 
date: 2018-12-17 00:00:00
---

因为一些奇奇怪怪的原因,原先定于1106进行的分享活动被临时改到了801进行,可以看到分享前20分钟教室已经坐满了人(于是后面来的小伙伴们都是靠着空调坐的)。

和往常一样,寒暄一阵到了六点过几分钟的样子就开始正式的分享活动啦~

![](https://mmbiz.qpic.cn/mmbiz_png/ErNIAficWks0hosnbR9m8htEoKPZzavxACgAPpzfuLDgxXYHdqDcuVwLA1MvIGwh23icFFMC3sn1nqepSUHOSBIg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

开场前的入座情况

本次分享的主讲人是帆布学长，先扯了扯最近北大的二值网络的情况。然后由打砖块的agent来引出强化学习的惩罚和奖励(reward function)的概念。

![图为agent逆天操作+反向一波流的瞬间](https://mmbiz.qpic.cn/mmbiz_png/ErNIAficWks0hosnbR9m8htEoKPZzavxAE6crkst4iarK6txZ9vHwH3QMuHbPiaRxAuZ4LqyuETIL8esbbqSKBEUw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

和在场的同学们讨论了强化学习和普通机器学习的分类机制的区别。

![和主讲人积极交流的同学们](https://mmbiz.qpic.cn/mmbiz_png/ErNIAficWks0hosnbR9m8htEoKPZzavxAAu6dXvibtWteibbrF1aVlgjyJ08f8dNwysHMdMAmmAwZqicYJgpgExqXQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)


小插曲1

😉帆布：强化学习和我们之前做的识别或者是检测，说白了就是分类。目标检测的候选框(yolo之前)，还在RCN时代的时候，比方说我要检测21个东西，他就要提出21个候选框，然后在这21个候选框里面分类，现在的大部分其实还是这样的，比如强化学习使用神经网络的这部分。

😖鸭鸭：那生成模型呢？(警觉

😳帆布：哎闭嘴>///< (笑)我是说做的这种决策，你生成模型做的不是决策对吧，我就是说的，做决策的这一类，它大部分都是做的分类嘛。哎呀不要打扰>< 好讨...好难受><！被鸭神戳穿了~

主讲人为现场非相关方向的同学补了下马尔科夫过程等基础知识。(虽然我真的一时半会听不懂 笑)

之后就是Q-Learning算法的介绍，进而聊到了DQN的细节内容-它引入了回放(replay)功能，并且使用了深度学习的函数逼近器，用深度学习网络代替了求Q的过程，结合了深度和强化学习。

展示了4种不同实验算法在Compete Pong游戏(某21分制的乒乓球)下的表现,不过由于运行不能，所以没能现场演示，只提供了最终的数据对比表格。

### 小插曲2

帆布：在座的各位有不用pytorch的吗？

Sk：我！tensorflow！

帆布：ok，well...

（实际在场用pytorch的只有帆布+鸭鸭+龙龙233）

现场review了DQN的代码+跑演示 (让棍子不倒下的游戏)

review了A3C的代码+跑演示 还是这个神奇的游戏x


### 小插曲3

    在尝试解释为什么强化学习没能在工控领域发光发热时...

帆布：让机器人拿锤子去敲钉子。目标是让锤子去敲,结果那个机器人学会了把锤子扔掉，拿自己的手去敲(笑，因为它发现，这么敲更快。

帆布：然后又发生了第二件事情，实验人员调整那个reward function嘛，他把那个reward function调整成了拿起锤子是有奖励的。然后它就会,先拿起锤子-放下-然后再敲。他这个就...就...对吧 (sk：就偏离了实际生产...哈哈哈哈 ) 他这个想法是很奇特的。

Wsy：这个机器人知道自己的手是什么做的吗...

帆布:不知道呀 

Wsy:那他怎么知道自己的手会比锤子更硬的?

帆布：试了呀！(笑)

帆布：强化学习就是试呀，像刚才那个棍子一样。它用锤子敲一下，reward是正的，那个钉子进去了。他用自己的手敲一下，发现也是个正的reward，而且更大，那他就用这个了呀，对吧？(笑)

![(笑成一片的一众基佬们,应部分人员要求,打了点码)](https://mmbiz.qpic.cn/mmbiz_png/ErNIAficWks0hosnbR9m8htEoKPZzavxA3uNFSHrL1BD5rlUdJqL7lpiadeFKw6PBTYg5BuQYXscXqYy9Kz4ZyPw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

之后的闲聊：

帆布：李飞飞的弟子用强化学习代替采样(视频检测)，不同于其他的采样方法 他用强化学习去选择抓哪帧。

可惜我没复现出来。

做的是动作检测，超过了一般的计算率方法的,高了几个百分点。

这个东西的贡献主要是这么一波操作只用了这个视频2%的数据。


主讲人还就DQN和A3C一事提到了如下的特性(坑点)：

训练这个东西，没有一个评价方法：为什么我的比你的好?

你只能说，我这个方法和他比 我得分高,我训练时间少

比如DQN和A3C 一个训练400个。一个4000个，最后分差不多

DQN十几分钟 A3C几分钟就完成了

但是从游戏回合上来说，A3C比DQN多，但是从时间来说,DQN又比A3C多

现在大家都是画的游戏得分曲线

顺便提一句，Ti8大家都看了吧? openAI5-就很弱智


最后帆布给相关方向的孩子们安利了莫烦python： https://morvanzhou.github.io/

也尝试拉其他不是相关方向的孩子们入坑强化学习(因为据说真的很好玩=w=)

![(结束后的现场 大家还在分别抱团探讨相关问题)](https://mmbiz.qpic.cn/mmbiz_png/ErNIAficWks0hosnbR9m8htEoKPZzavxAhPPFB8Wvmt5ibsObicW3Z3ILkRpYCDQFlK8picia0jAlL4h0wDx8KlIJvA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

| 附录 |
| :------- |
|[本次活动的PPT&代码](https://pan.baidu.com/s/1ljemks5474Xe0Kx7cv_e6A)|

